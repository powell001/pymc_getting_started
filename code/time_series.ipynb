{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import pymc.sampling_jax\n",
    "import numpyro\n",
    "import blackjax\n",
    "import jax\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import arviz as az\n",
    "\n",
    "settings = {'figure.figsize':(14,4),\n",
    "            'figure.dpi':144,\n",
    "            'figure.facecolor':'w',\n",
    "            'axes.spines.top':False,\n",
    "            'axes.spines.bottom':False,\n",
    "            'axes.spines.left':False,\n",
    "            'axes.spines.right':False,\n",
    "            'axes.grid':True,\n",
    "            'grid.linestyle':'--',\n",
    "            'grid.linewidth':0.5}\n",
    "plt.rcParams.update(settings)\n",
    "\n",
    "SEED = sum(map(ord, 'Forcasting in PyMC'))\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(idata, test_data=None, oos_name='Test'):\n",
    "    groups = ['posterior_predictive', 'predictions', 'observed_data']\n",
    "    colors = ['tab:blue', 'tab:orange', 'tab:red']\n",
    "    labels = ['Training', oos_name, 'Data']\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    time_dim_name = list(idata.observed_data.dims.keys())[0]\n",
    "    x_train = idata.observed_data.coords[time_dim_name]\n",
    "    \n",
    "    H = list(idata.predictions.dims.values())[-1]\n",
    "    x_test = np.arange(x_train[-1], x_train[-1] + H)\n",
    "    \n",
    "    x_values = [x_train, x_test, x_train]\n",
    "    \n",
    "    for group, color, label, x_val in zip(groups, colors, labels, x_values):\n",
    "        data = getattr(idata, group).y_hat\n",
    "        \n",
    "        if group == 'observed_data':\n",
    "            ax.plot(x_val, data, c=color, label=label)\n",
    "            continue\n",
    "            \n",
    "        hdi = az.hdi(data).y_hat\n",
    "        ax.plot(x_val, data.mean(dim=['chain', 'draw']), label=label)\n",
    "        ax.fill_between(x_val, *hdi.values.T, color=color, alpha=0.25)\n",
    "    if test_data is not None:\n",
    "        ax.plot(x_test, test_data, color='tab:red')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alpha = 1\n",
    "true_gamma = 0.2\n",
    "true_sigma = 1.5\n",
    "T = 100\n",
    "\n",
    "noise = rng.normal(scale=true_sigma, size=T)\n",
    "t = np.arange(T)\n",
    "data = true_alpha + true_gamma * t + noise\n",
    "train_data = data[:90]\n",
    "test_data = data[-10:]\n",
    "\n",
    "plt.plot(train_data, color='tab:blue')\n",
    "plt.plot(np.arange(90, 100), test_data, ls='--', color='tab:red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as det_trend:\n",
    "    t_pt = pm.MutableData('t', t[:90])\n",
    "    alpha = pm.Normal('alpha')\n",
    "    gamma = pm.Normal('gamma')\n",
    "    \n",
    "    mu = pm.Deterministic('mu', alpha + gamma * t_pt)\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    y_hat = pm.Normal('y_hat', mu=mu, sigma=sigma, observed=train_data, shape=t_pt.shape)\n",
    "    idata = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with det_trend:\n",
    "    #in-sample \n",
    "    idata = pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n",
    "    \n",
    "    #out-of-sample\n",
    "    pm.set_data({'t':t[-10:]})\n",
    "    idata = pm.sample_posterior_predictive(idata, extend_inferencedata=True, predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(idata, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'../data/hsales.csv')\n",
    "\n",
    "df['index'] = pd.date_range(\"1973\", periods=275, freq='M').strftime('%Y-%m')\n",
    "df.set_index('index', inplace=True)\n",
    "\n",
    "# Center and scale the data\n",
    "df['hsales'] = df['hsales'].transform(lambda x: (x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(legend=False, title='Sales of new one-family houses, USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_piecewise_trend(t, t_max, n_changepoints):    \n",
    "    s = pt.linspace(0, t_max, n_changepoints+2)[1:-1]\n",
    "    A = (t[:, None] > s)*1\n",
    "    \n",
    "    return A, s\n",
    "\n",
    "def create_fourier_features(t, n, p=365.25):\n",
    "    x = 2 * np.pi * (pt.arange(n)+1) * t[:, None] / p\n",
    "    return pt.concatenate((pt.cos(x), pt.sin(x)), axis = 1)\n",
    "\n",
    "def generate_features(t, t_max, n_changepoints=10, n_fourier=6, p=365.25):\n",
    "    A, s = create_piecewise_trend(t, t_max, n_changepoints)\n",
    "    X = create_fourier_features(t, n_fourier, p)\n",
    "    \n",
    "    return A, s, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(df.shape[0])\n",
    "t_max = max(t)\n",
    "with pm.Model() as prophet_model:\n",
    "    t_pt = pm.MutableData('t', t)\n",
    "    A, s, X = generate_features(t_pt, t_max, n_changepoints=10, n_fourier=6, p=12)\n",
    "    \n",
    "    initial_slope = pm.Normal('initial_slope')\n",
    "    initial_intercept = pm.Normal('initial_intercept')\n",
    "    \n",
    "    # n_changepoint offsets terms to build the peicewise trend\n",
    "    deltas = pm.Normal('offset_delta', shape=(10,))\n",
    "        \n",
    "    intercept = initial_intercept + ((-s * A) * deltas).sum(axis=1)\n",
    "    slope = initial_slope + (A * deltas).sum(axis=1)\n",
    "    \n",
    "    # n_fourier * 2 seasonal coefficients\n",
    "    beta = pm.Normal('beta', size=12)\n",
    "    \n",
    "    mu = pm.Deterministic('mu', intercept + slope * t_pt + X @ beta)\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    y_hat = pm.Normal('y_hat', mu=mu, sigma=sigma, observed=df.values.ravel(), shape=t_pt.shape)\n",
    "    \n",
    "    idata = pymc.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with prophet_model:\n",
    "    #in-sample \n",
    "    idata = pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n",
    "    \n",
    "    #out-of-sample\n",
    "    last_t = t[-1]\n",
    "    \n",
    "    # Forcast 3 years of home sales\n",
    "    forcast_t = np.arange(last_t, last_t + 36)\n",
    "    pm.set_data({'t':forcast_t})\n",
    "    idata = pm.sample_posterior_predictive(idata, extend_inferencedata=True, predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(idata, oos_name='Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sigma = 0.1\n",
    "noise_sigma = 0.1\n",
    "T = 100\n",
    "\n",
    "innovations = rng.normal(scale=true_sigma, size=T)\n",
    "noise = rng.normal(scale=noise_sigma, size=T)\n",
    "data = innovations.cumsum() + noise\n",
    "\n",
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as grw:\n",
    "    sigma_innov = pm.Exponential('sigma_innov', 1)\n",
    "    innovs = pm.Normal('innovations', sigma=sigma_innov, size=100)\n",
    "    \n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    y_hat = pm.Normal('y_hat', mu=innovs.cumsum(), sigma=sigma, observed=data)\n",
    "    \n",
    "    idata = pm.sample(init='jitter+adapt_diag_grad', target_accept=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the easy part, in-sample prediction\n",
    "with grw:\n",
    "    idata = pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n",
    "\n",
    "n_chains, n_draws, *_ = idata.posterior_predictive.y_hat.values.shape\n",
    "# Next simulate the recursive model\n",
    "H = 25\n",
    "simulations = np.empty((n_chains, n_draws, H))\n",
    "x0 = data[-1]\n",
    "\n",
    "simulations[:, :, 0] = x0\n",
    "\n",
    "# simulate the forward model\n",
    "for t in range(1, H):\n",
    "    simulations[:, :, t] = simulations[:, :, t-1] + rng.normal(scale=idata.posterior.sigma_innov) + rng.normal(scale=idata.posterior.sigma)\n",
    "\n",
    "    \n",
    "# Add the simulations to the idata in a \"predictions\" group\n",
    "# Congrats, you just did a posterior predictive sampling by hand\n",
    "import xarray as xr\n",
    "idata.add_groups({'predictions':xr.Dataset(data_vars={'y_hat':(['chain', 'draw', 'y_hat_dim_2'], simulations)},\n",
    "                                           coords={'chain':np.arange(n_chains),\n",
    "                                                   'draw':np.arange(n_draws), \n",
    "                                                   'y_hat_dim_2':np.arange(T, T+H)})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(idata, oos_name='Forecasts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_data = np.r_[data, np.full(H, np.nan)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m sigma \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mExponential(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_hat\u001b[39m\u001b[38;5;124m'\u001b[39m, mu\u001b[38;5;241m=\u001b[39minnovs\u001b[38;5;241m.\u001b[39mcumsum(), sigma\u001b[38;5;241m=\u001b[39msigma, observed\u001b[38;5;241m=\u001b[39mextended_data)\n\u001b[0;32m---> 10\u001b[0m idata \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjitter+adapt_diag_grad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m idata \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39msample_posterior_predictive(idata, extend_inferencedata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pymc_env/lib/python3.12/site-packages/pymc/sampling/mcmc.py:875\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pymc_env/lib/python3.12/site-packages/pymc/sampling/mcmc.py:906\u001b[0m, in \u001b[0;36m_sample_return\u001b[0;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[0;32m--> 906\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pymc_env/lib/python3.12/site-packages/pymc/backends/base.py:593\u001b[0m, in \u001b[0;36m_choose_chains\u001b[0;34m(traces, tune)\u001b[0m\n\u001b[1;32m    591\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    595\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[1;32m    596\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "extended_data = np.r_[data, np.full(H, np.nan)]\n",
    "\n",
    "with pm.Model() as grw:\n",
    "    sigma_innov = pm.Exponential('sigma_innov', 1)\n",
    "    innovs = pm.Normal('innovations', sigma=sigma_innov, size=100 + H)\n",
    "    \n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    y_hat = pm.Normal('y_hat', mu=innovs.cumsum(), sigma=sigma, observed=extended_data)\n",
    "    \n",
    "    idata = pm.sample(init='jitter+adapt_diag_grad', target_accept=0.99)\n",
    "    idata = pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hdi = az.hdi(idata.posterior_predictive.y_hat).y_hat\n",
    "mu = idata.posterior_predictive.y_hat.mean(dim=['chain', 'draw']).values\n",
    "ax.plot(data, color='tab:red', label='Data')\n",
    "ax.plot(mu[:T], color='tab:blue', label='Training')\n",
    "ax.fill_between(np.arange(T), *hdi.values.T[:, :T], alpha=0.25, color='tab:blue')\n",
    "\n",
    "ax.plot(np.arange(T, T+H), mu[T:], color='tab:orange', label='Forecasts')\n",
    "ax.fill_between(np.arange(T, T+H), *hdi.values.T[:, T:], alpha=0.25, color='tab:orange')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
